---
layout: post
title: "Spark 2.2 资源动态调整"
subtitle: "Spark 2.2 Cpu/Mem dynamic "
author: "et"
header-img: "img/post-bg-spark22.jpg"
header-mask: 0.4
tags:
  - Spark
  - CDH
  - 大数据
  - yarn
---

<br>
>spark 动态资源分配  yarn 配置

<br>
<br>

 cloudera manager 中配置 yarn-site.xml (yarn-site.xml 的 YARN 服务高级配置代码段 (安全阀))

>        <property>
>          <name>yarn.nodemanager.aux-services</name>
>          <value>mapreduce_shuffle,spark_shuffle</value>
>        </property>
>        <property>
>          <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
>          <value>org.apache.spark.network.yarn.YarnShuffleService</value>
>        </property>
>        <property>
>          <name>spark.shuffle.service.port</name>
>          <value>7337</value>
>        </property>     


 放置对应版本的 jar 到各个nodemanager 节点:

        cd  /opt/cloudera/parcels/CDH/lib/hadoop-yarn/lib
        cp  spark-2.2.0.cloudera1-yarn-shuffle.jar  /opt/cloudera/parcels/CDH/lib/hadoop-yarn/lib/

 配置spark 运行参数

        spark.shuffle.service.enabled true //启用External shuffle Service服务
        spark.shuffle.service.port 7337 //Shuffle Service服务端口，必须和yarn-site中的一致
        spark.dynamicAllocation.enabled true //开启动态资源分配
        spark.dynamicAllocation.minExecutors 1 //每个Application最小分配的executor数
        spark.dynamicAllocation.maxExecutors 30 //每个Application最大并发分配的executor数
        spark.dynamicAllocation.schedulerBacklogTimeout 1s
        spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s

   或者在启动流任务时:

        nohup spark2-submit --master yarn --queue realtime --jars ${extJar} --conf "spark.dynamicAllocation.initialExecutors=2" --executor-cores 2   --conf "spark.ui.port=$uiPort" --conf "spark.shuffle.service.enabled=true" --conf "spark.shuffle.service.prot=7337" --conf "spark.dynamicAllocation.enabled=true" --conf "spark.eventLog.enabled=false" --conf "spark.task.maxFailures=30" --conf "spark.streaming.kafka.maxRatePerPartition=500" --conf "spark.streaming.backpressure.enabled=true"   --conf "spark.serializer=org.apache.spark.serializer.KryoSerializer"  --conf "spark.executor.heartbeatInterval=420000" --conf "spark.network.timeout=480000"  --class ${package}.$1 $2 $arg1 $arg2 $arg3 >$1.log 2>&1 &


 重启yarn 各 NodeManager 



<br>

遇到问题:
<br>
    任务起来后，无法连接 kafka  ,最后发现 spark2 连接kafka 默认使用的客户端版本不对，我们换成 0.10  后，重启 spark2     
    具体是在: cdh(cm) --> Spark 2  组件 --> 配置 ,里面搜索 “SPARK_KAFKA”, 把 spark_kafka_version 的值 设置成 “0.10” 版本       

 
